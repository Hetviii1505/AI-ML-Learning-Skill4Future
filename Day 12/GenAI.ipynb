{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92c131ec",
   "metadata": {},
   "source": [
    "### RNN\n",
    "Recurrent Neural Networks (RNNs) are a class of artificial neural networks designed to recognize patterns in sequences of data, such as time series, speech, text, or video. Unlike traditional feedforward neural networks, RNNs have connections that form directed cycles, allowing them to maintain a 'memory' of previous inputs in the sequence.\n",
    " \n",
    "Types of RNNs include:\n",
    "- **One to One**: Standard neural networks where each input corresponds to one output.\n",
    "- **One to Many**: A single input produces a sequence of outputs (e.g., image captioning).\n",
    "- **Many to One**: A sequence of inputs produces a single output (e.g., sentiment analysis).\n",
    "- **Many to Many**: Both input and output are sequences (e.g., machine translation).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f985281",
   "metadata": {},
   "source": [
    "### NLP\n",
    "Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human language. It involves the development of algorithms and models that enable machines to understand, interpret, generate, and respond to human language in a meaningful way. NLP encompasses a wide range of tasks, including text classification, sentiment analysis, machine translation, named entity recognition, and more. Techniques used in NLP include tokenization, part-of-speech tagging, parsing, and the use of advanced models like transformers and RNNs to capture the context and semantics of language.\n",
    "\n",
    "NLP is divided into two main categories:\n",
    "- **Natural Language Understanding (NLU)**: This involves comprehending the meaning and context of text, including tasks like sentiment analysis, entity recognition, and intent detection.\n",
    "- **Natural Language Generation (NLG)**: This focuses on producing human-like text based on given data or prompts, such as in chatbots, text summarization, and content creation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4bcd26",
   "metadata": {},
   "source": [
    "Applications of NLP include:\n",
    "- **Chatbots and Virtual Assistants**: Systems like Siri, Alexa, and Google Assistant that interact with users in natural language.\n",
    "- **Sentiment Analysis**: Analyzing customer reviews or social media posts to determine public sentiment towards products or services.\n",
    "- **Machine Translation**: Translating text from one language to another, as seen in services like Google Translate.\n",
    "- **Text Summarization**: Automatically generating concise summaries of longer documents.\n",
    "\n",
    "NLP Python packages:\n",
    "- **NLTK (Natural Language Toolkit)**: A comprehensive library for building NLP applications, providing tools for tokenization, stemming, tagging, parsing, and more.\n",
    "- **spaCy**: An open-source library designed for fast and efficient NLP tasks, including named entity recognition, part-of-speech tagging, and dependency parsing.\n",
    "- **Transformers (by Hugging Face)**: A library that provides pre-trained models for various NLP tasks, including BERT, GPT, and T5, enabling state-of-the-art performance in text generation, classification, and more.\n",
    "- **Gensim**: A library focused on topic modeling and document similarity analysis, useful for tasks like word embedding and latent semantic analysis.\n",
    "- **TextBlob**: A simple library for processing textual data, providing easy-to-use APIs for common NLP tasks like sentiment analysis and noun phrase extraction.\n",
    "\n",
    "How Text Preprocessing in NLP works ?\n",
    "- Tokenization\n",
    "- Stop Words Removal\n",
    "- Normalization (Lowercasing, Stemming, Lemmatization)\n",
    "- Vectorization (Bag of Words, TF-IDF, Word Embeddings)\n",
    "- Part-of-Speech Tagging\n",
    "- Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a6cc5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There', 'are', 'multiple', 'ways', 'we', 'can', 'perform', 'tokenization', 'on', 'given', 'text', 'data.', 'We', 'can', 'choose', 'any', 'method', 'based', 'on', 'langauge,', 'library', 'and', 'purpose', 'of', 'modeling.']\n"
     ]
    }
   ],
   "source": [
    "# word tokenization \n",
    "\n",
    "text = \"There are multiple ways we can perform tokenization on given text data. We can choose any method based on langauge, library and purpose of modeling.\"\n",
    "# Split text by whitespace\n",
    "tokens = text.split()\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f56debd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Characters like periods, exclamation point and newline char are used to separate the sentences',\n",
       " 'But one drawback with split() method, that we can only use one separator at a time! So sentence tonenization wont be foolproof with split() method.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets split the given text by full stop (.)\n",
    "text = \"Characters like periods, exclamation point and newline char are used to separate the sentences. But one drawback with split() method, that we can only use one separator at a time! So sentence tonenization wont be foolproof with split() method.\"\n",
    "text.split(\". \") # Note the space after the full stop makes sure that we dont get empty element at the end of list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d71a3f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\asys\\appdata\\roaming\\python\\python313\\site-packages (3.9.2)\n",
      "Requirement already satisfied: click in c:\\users\\asys\\anaconda3\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\asys\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\asys\\anaconda3\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\asys\\anaconda3\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\asys\\appdata\\roaming\\python\\python313\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --user -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "110033bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac0afb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aede564b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Asys\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Asys\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "475bd91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There', 'are', 'multiple', 'ways', 'we', 'can', 'perform', 'tokenization', 'on', 'given', 'text', 'data', '.', 'We', 'can', 'choose', 'any', 'method', 'based', 'on', 'langauge', ',', 'library', 'and', 'purpose', 'of', 'modeling', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"There are multiple ways we can perform tokenization on given text data. We can choose any method based on langauge, library and purpose of modeling.\"\"\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "465a5562",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "962fb051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Characters like periods, exclamation point and newline char are used to separate the sentences.',\n",
       " 'But one drawback with split() method, that we can only use one separator at a time!',\n",
       " 'So sentence tonenization wont be foolproof with split() method.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "text = \"\"\"Characters like periods, exclamation point and newline char are used to separate the sentences. But one drawback with split() method, that we can only use one separator at a time! So sentence tonenization wont be foolproof with split() method.\"\"\"\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "386ca769",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82431246",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Asys\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5b0573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Sentence \n",
      "\n",
      "He determined to drop his litigation with the monastry , and relinguish his claims to the wood-cuting and fishery rihgts at once . He was the more ready to do this becuase the rights had become much less valuable , and he had indeed the vaguest idea where the wood and river in question were .\n",
      "\n",
      "Filtered Sentence \n",
      "\n",
      "He determined drop litigation monastry , relinguish claims wood-cuting fishery rihgts . He ready becuase rights become much less valuable , indeed vaguest idea wood river question .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "len(set(stopwords.words('english')))\n",
    "\n",
    "# sample sentence\n",
    "text = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \n",
    "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \n",
    "indeed the vaguest idea where the wood and river in question were.\"\"\"\n",
    "\n",
    "# set of stop words\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "# tokens of words  \n",
    "word_tokens = word_tokenize(text) \n",
    "    \n",
    "filtered_sentence = [] \n",
    "  \n",
    "for w in word_tokens: \n",
    "    \n",
    "    if w not in stop_words: \n",
    "        filtered_sentence.append(w) \n",
    "\n",
    "\n",
    "print(\"\\nOriginal Sentence \\n\")\n",
    "print(\" \".join(word_tokens)) \n",
    "\n",
    "\n",
    "print(\"\\nFiltered Sentence \\n\")\n",
    "print(\" \".join(filtered_sentence)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3120b76",
   "metadata": {},
   "source": [
    "### GenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b954585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, BatchNormalization, Reshape, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5f9dd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator model\n",
    "def build_generator():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=100))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(1024))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(28 * 28, activation='tanh'))\n",
    "    model.add(Reshape((28, 28, 1)))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "768d7842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator model\n",
    "def build_discriminator():\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(28, 28, 1)))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9eb18c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the GAN\n",
    "def compile_gan(generator, discriminator):\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
    "    discriminator.trainable = False\n",
    "    gan_input = tf.keras.Input(shape=(100,))\n",
    "    gan_output = discriminator(generator(gan_input))\n",
    "    gan = tf.keras.Model(gan_input, gan_output)\n",
    "    gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
    "    return gan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e282432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the GAN\n",
    "def train_gan(generator, discriminator, gan, epochs=10000, batch_size=64, sample_interval=1000):\n",
    "    (X_train, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "    X_train = np.expand_dims(X_train, axis=3)\n",
    "    real = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "        real_imgs = X_train[idx]\n",
    "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "        gen_imgs = generator.predict(noise)\n",
    "        d_loss_real = discriminator.train_on_batch(real_imgs, real)\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "        g_loss = gan.train_on_batch(noise, real)\n",
    "\n",
    "        if epoch % sample_interval == 0:\n",
    "            print(f\"{epoch} [D loss: {d_loss[0]}, acc.: {100 * d_loss[1]}%] [G loss: {g_loss}]\")\n",
    "            sample_images(generator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f64260b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
