{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8ec1812",
   "metadata": {},
   "source": [
    "### ENSEMBLE MACHINE LEARNING MODELS\n",
    "Ensemble machine learning models combine multiple individual models to improve overall performance, accuracy, and robustness. By aggregating the predictions of several models, ensemble methods can reduce overfitting, increase generalization, and enhance predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd32f57a",
   "metadata": {},
   "source": [
    "Techniques such as bagging, boosting, and stacking are commonly used to create ensemble models.\n",
    "\n",
    "Bagging \n",
    "- used to reduce variance by training multiple models on different subsets of the data and averaging their predictions (e.g., Random Forests).\n",
    "\n",
    "Boosting\n",
    "- Boostion fit a sequence of weak learners to create a strong learner.\n",
    "- focuses on reducing bias by sequentially training models, where each model attempts to correct the errors of its predecessor (e.g., AdaBoost, Gradient Boosting).\n",
    "\n",
    "Stacking\n",
    "- combines multiple models by training a meta-model to make final predictions based on the outputs of base models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d8306e",
   "metadata": {},
   "source": [
    "Random Forest\n",
    "- An ensemble method that constructs multiple decision trees during training and outputs the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n",
    "- A ramdom sample of m predictors is selected as split candidates from the full set of p predictors at each node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d09638",
   "metadata": {},
   "source": [
    "Q. diifference between Bagging and Boosting?\n",
    "\n",
    "Q. what is bootstrapping?\n",
    "        row sampling with replacement from the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ace07a",
   "metadata": {},
   "source": [
    "Gradient Boosting Technique\n",
    "- Builds models sequentially, with each new model correcting the errors of the previous ones.\n",
    "- Focuses on reducing bias and improving predictive accuracy.\n",
    "\n",
    "Sequentially fiting \n",
    "- A model is trained on the original data.\n",
    "- The residual errors from the first model are calculated.\n",
    "- A second model is trained on these residuals to capture patterns not learned by the first model.\n",
    "- This process is repeated for a specified number of iterations or until convergence.\n",
    "\n",
    "XGBoost (Extreme Gradient Boosting)\n",
    "- An extension of gradient boosting with following enhancements:\n",
    "    - Regularization: Includes L1 and L2 regularization to prevent overfitting.\n",
    "    - Parallel Processing: Utilizes parallel computing for faster training.\n",
    "    - Handling Missing Values: Efficiently manages missing data during training.\n",
    "    - Tree Pruning: Implements a more effective tree pruning algorithm to improve model performance.\n",
    "    - Built-in Cross-Validation: Provides built-in support for cross-validation during training.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fc3083",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
